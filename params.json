{
  "name": "Parallel-Poisson-Solver",
  "tagline": "Poisson equation solver written in C++ with distributed memory parallelization using MPI",
  "body": "#Poisson Equation Solver\r\n\r\nParallel in-house Solvers:\r\n* Jacobi solver\r\n* Red-Black Gauss-Seidel with successive over relaxation\r\n* Steepest descent\r\n* Conjugate gradient\r\n\r\nCode style:\r\n* Written using OOPS in C++.\r\n\r\nUsage:\r\n* To compile parallel solver on local machine machine/cluster: make\r\n* changed - (To compile parallel solver on Cray supercomputer: CC -O3 parallel_poisson.c -o run)\r\n* To run on cluster: qsub pp.sh\r\n* changed - (To run on Cray supercomputer: qsub submit.sh) \r\n* Type plot_output within an octave terminal to see the surface plot of the output or run the python script plot_output.py\r\n\r\nChangelog:\r\n* Parallelization on distributed memory system using MPI.\r\n* Domain decomposition is done.\r\n* Communication of buffer cells is working now.\r\n* Global residual added now.\r\n* Speedup tested upto 16 processes on local machine.\r\n* Speedup tested upto 720 processes on Cray XC40.\r\n* Red-Black Gauss-Seidel solver added.\r\n\r\nThings to do:\r\n* Parllelize Bi-Conjugate Gradient STABilized solver.\r\n* Non-blocking communication is to be tested.\r\n* update report.pptx\r\n* Mesh importing package to be added\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}